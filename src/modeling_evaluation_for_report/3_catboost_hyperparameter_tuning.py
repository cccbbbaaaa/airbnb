"""
================================================================================
CatBoost è¶…å‚æ•°è°ƒä¼˜
CatBoost Hyperparameter Tuning
================================================================================
æœ¬è„šæœ¬ç”¨äº:
1. æµ‹è¯•ä¸åŒè¿­ä»£æ¬¡æ•°çš„CatBoostæ¨¡å‹ (550, 600, 650)
2. è¿›è¡Œç½‘æ ¼æœç´¢å¯»æ‰¾æœ€ä¼˜è¶…å‚æ•°ç»„åˆ
3. ä¿å­˜æµ‹è¯•ç»“æœå’Œæœ€ä¼˜æ¨¡å‹

This script performs:
1. Testing CatBoost with different iteration counts (550, 600, 650)
2. Grid search for optimal hyperparameter combination
3. Saving test results and optimal model

Author: Data Science Course Project
Date: 2025-11-27
================================================================================
"""

import numpy as np
import pandas as pd
from pathlib import Path
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score, accuracy_score, f1_score
from sklearn.model_selection import train_test_split
import time

# ==================== PROJECT PATHS ====================
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "processed"
MODEL_DIR = PROJECT_ROOT / "charts" / "charts_for_report" / "modeling"
MODEL_DIR.mkdir(parents=True, exist_ok=True)


def load_and_prepare_data():
    """
    Load feature-engineered data and prepare train/test split

    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    print("\n" + "=" * 80)
    print("åŠ è½½ç‰¹å¾æ•°æ® - LOADING FEATURE DATA")
    print("=" * 80)

    data_path = DATA_DIR / "train_data.csv"
    df = pd.read_csv(data_path)

    # Filter valid ratings and create binary target
    working_df = df[df["review_scores_rating"].notna()].copy()
    working_df["is_five_star"] = np.isclose(
        working_df["review_scores_rating"], 5.0, atol=1e-6
    ).astype(int)

    # Separate features and target
    exclude_cols = {"review_scores_rating", "is_five_star"}
    feature_cols = [col for col in working_df.columns if col not in exclude_cols]
    X = working_df[feature_cols].copy()
    y = working_df["is_five_star"].copy()

    # Train-test split (same as in training script)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"\næ•°æ®é›†åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {X_train.shape}")
    print(f"  æµ‹è¯•é›†: {X_test.shape}")
    print(f"  ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"  5æ˜Ÿæ ·æœ¬æ¯”ä¾‹: {y.mean():.2%}")

    return X_train, X_test, y_train, y_test


def test_iteration_counts(X_train, X_test, y_train, y_test,
                         iterations_list=[550, 600, 650]):
    """
    Test CatBoost with different iteration counts

    Args:
        X_train, X_test, y_train, y_test: Training and test data
        iterations_list: List of iteration counts to test

    Returns:
        pd.DataFrame: Results comparison
    """
    print("\n" + "=" * 80)
    print("æµ‹è¯•ä¸åŒè¿­ä»£æ¬¡æ•° - TESTING DIFFERENT ITERATION COUNTS")
    print("=" * 80)
    print(f"\nå°†æµ‹è¯•çš„è¿­ä»£æ¬¡æ•°: {iterations_list}")

    results = []

    for iterations in iterations_list:
        print(f"\n{'=' * 80}")
        print(f"æµ‹è¯•è¿­ä»£æ¬¡æ•°: {iterations}")
        print(f"{'=' * 80}")

        # Train model
        print(f"\nè®­ç»ƒ CatBoost (iterations={iterations})...")
        start_time = time.time()

        model = CatBoostClassifier(
            iterations=iterations,
            learning_rate=0.1,
            depth=6,
            l2_leaf_reg=3,
            random_seed=42,
            verbose=False,
            eval_metric='AUC',
            auto_class_weights='Balanced'
        )

        model.fit(X_train, y_train)
        train_time = time.time() - start_time

        # Predictions
        y_train_pred_proba = model.predict_proba(X_train)[:, 1]
        y_test_pred_proba = model.predict_proba(X_test)[:, 1]

        # Metrics
        train_auc = roc_auc_score(y_train, y_train_pred_proba)
        test_auc = roc_auc_score(y_test, y_test_pred_proba)

        # Convert to error percentage
        train_error = (1 - train_auc) * 100
        test_error = (1 - test_auc) * 100
        gap = test_error - train_error

        # Additional metrics
        y_test_pred = model.predict(X_test)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        test_f1 = f1_score(y_test, y_test_pred)

        print(f"\nç»“æœ:")
        print(f"  è®­ç»ƒæ—¶é—´: {train_time:.2f}s")
        print(f"  Train AUC: {train_auc:.4f} (Error: {train_error:.2f}%)")
        print(f"  Test AUC:  {test_auc:.4f} (Error: {test_error:.2f}%)")
        print(f"  Gap (Test - Train): {gap:.2f}%")
        print(f"  Test Accuracy: {test_accuracy:.4f}")
        print(f"  Test F1: {test_f1:.4f}")

        results.append({
            'iterations': iterations,
            'train_auc': train_auc,
            'test_auc': test_auc,
            'train_error': train_error,
            'test_error': test_error,
            'gap': gap,
            'test_accuracy': test_accuracy,
            'test_f1': test_f1,
            'train_time': train_time
        })

        # Save model
        model_path = MODEL_DIR / f"catboost_model_{iterations}iter.cbm"
        model.save_model(model_path)
        print(f"  æ¨¡å‹å·²ä¿å­˜: {model_path}")

    # Create comparison table
    print("\n" + "=" * 80)
    print("ç»“æœå¯¹æ¯” - COMPARISON")
    print("=" * 80)

    results_df = pd.DataFrame(results)
    print("\nè¯¦ç»†å¯¹æ¯”:")
    print(results_df.to_string(index=False))

    # Determine optimal
    optimal_idx = results_df['test_auc'].idxmax()
    optimal_config = results_df.loc[optimal_idx]

    print("\n" + "=" * 80)
    print("æœ€ä¼˜é…ç½® - OPTIMAL CONFIGURATION")
    print("=" * 80)
    print(f"\nğŸ¯ æœ€ä¼˜è¿­ä»£æ¬¡æ•°: {int(optimal_config['iterations'])}")
    print(f"   Test AUC: {optimal_config['test_auc']:.4f}")
    print(f"   Test Error: {optimal_config['test_error']:.2f}%")
    print(f"   Gap: {optimal_config['gap']:.2f}%")
    print(f"   Test F1: {optimal_config['test_f1']:.4f}")
    print(f"   è®­ç»ƒæ—¶é—´: {optimal_config['train_time']:.2f}s")

    # Save results
    results_path = MODEL_DIR / "catboost_iteration_comparison.csv"
    results_df.to_csv(results_path, index=False)
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {results_path}")

    # Analysis
    print("\n" + "=" * 80)
    print("åˆ†æå»ºè®® - ANALYSIS")
    print("=" * 80)

    if optimal_config['iterations'] == min(iterations_list):
        print(f"\nğŸ“Š åˆ†æ: {int(optimal_config['iterations'])}æ¬¡è¿­ä»£å·²ç»è¾¾åˆ°æœ€ä¼˜")
        print(f"   æ›´å¤šè¿­ä»£å­˜åœ¨è½»å¾®è¿‡æ‹Ÿåˆ")
        print(f"   å»ºè®®: ä½¿ç”¨{int(optimal_config['iterations'])}æ¬¡è¿­ä»£ä½œä¸ºæœ€ç»ˆæ¨¡å‹")
    elif optimal_config['iterations'] == max(iterations_list):
        print(f"\nğŸ“Š åˆ†æ: {int(optimal_config['iterations'])}æ¬¡è¿­ä»£è¡¨ç°æœ€ä½³")
        print(f"   æ¨¡å‹ä»åœ¨æ”¹è¿›ä¸­")
        print(f"   å»ºè®®: ä½¿ç”¨{int(optimal_config['iterations'])}æ¬¡è¿­ä»£ï¼Œæˆ–è€ƒè™‘è¿›ä¸€æ­¥å¢åŠ è¿­ä»£æ¬¡æ•°")
    else:
        print(f"\nğŸ“Š åˆ†æ: {int(optimal_config['iterations'])}æ¬¡è¿­ä»£è¡¨ç°æœ€ä½³")
        print(f"   åœ¨æ€§èƒ½å’Œç¨³å®šæ€§ä¹‹é—´å–å¾—äº†è‰¯å¥½å¹³è¡¡")
        print(f"   å»ºè®®: ä½¿ç”¨{int(optimal_config['iterations'])}æ¬¡è¿­ä»£ä½œä¸ºæœ€ç»ˆæ¨¡å‹")

    # Check for overfitting trend
    if results_df['gap'].iloc[-1] > results_df['gap'].iloc[0]:
        print(f"\nâš ï¸  æ³¨æ„: éšç€è¿­ä»£æ¬¡æ•°å¢åŠ ï¼Œè¿‡æ‹Ÿåˆgapä» {results_df['gap'].iloc[0]:.2f}% å¢è‡³ {results_df['gap'].iloc[-1]:.2f}%")
    else:
        print(f"\nâœ… è‰¯å¥½: è¿‡æ‹Ÿåˆgapä¿æŒç¨³å®šæˆ–æ”¹å–„")

    return results_df


def perform_grid_search(X_train, X_test, y_train, y_test,
                       param_grid=None, cv_folds=3):
    """
    Perform grid search for optimal hyperparameters

    Args:
        X_train, X_test, y_train, y_test: Training and test data
        param_grid: Dictionary of parameters to search
        cv_folds: Number of cross-validation folds

    Returns:
        dict: Best parameters and results
    """
    print("\n" + "=" * 80)
    print("ç½‘æ ¼æœç´¢è¶…å‚æ•°è°ƒä¼˜ - GRID SEARCH HYPERPARAMETER TUNING")
    print("=" * 80)

    if param_grid is None:
        param_grid = {
            'iterations': [400, 550, 800],
            'depth': [4, 6, 8],
            'learning_rate': [0.01, 0.03, 0.05],
            'l2_leaf_reg': [1, 3, 5],
        }

    print(f"\nå‚æ•°æœç´¢ç©ºé—´:")
    for param, values in param_grid.items():
        print(f"  {param}: {values}")

    total_combinations = np.prod([len(v) for v in param_grid.values()])
    print(f"\næ€»ç»„åˆæ•°: {total_combinations}")
    print(f"äº¤å‰éªŒè¯æŠ˜æ•°: {cv_folds}")
    print(f"æ€»è®­ç»ƒæ¬¡æ•°: {total_combinations * cv_folds}")

    from sklearn.model_selection import GridSearchCV

    # Create base model
    base_model = CatBoostClassifier(
        random_seed=42,
        verbose=False,
        eval_metric='AUC',
        auto_class_weights='Balanced'
    )

    # Perform grid search
    print(f"\nå¼€å§‹ç½‘æ ¼æœç´¢...")
    start_time = time.time()

    grid_search = GridSearchCV(
        estimator=base_model,
        param_grid=param_grid,
        cv=cv_folds,
        scoring='roc_auc',
        n_jobs=-1,
        verbose=1
    )

    grid_search.fit(X_train, y_train)
    search_time = time.time() - start_time

    print(f"\nâœ… ç½‘æ ¼æœç´¢å®Œæˆï¼ç”¨æ—¶: {search_time:.2f}s ({search_time/60:.1f}åˆ†é’Ÿ)")

    # Best parameters
    print("\n" + "=" * 80)
    print("æœ€ä¼˜å‚æ•° - BEST PARAMETERS")
    print("=" * 80)
    print(f"\n{grid_search.best_params_}")

    # Best model performance
    best_model = grid_search.best_estimator_
    y_train_pred = best_model.predict_proba(X_train)[:, 1]
    y_test_pred = best_model.predict_proba(X_test)[:, 1]

    train_auc = roc_auc_score(y_train, y_train_pred)
    test_auc = roc_auc_score(y_test, y_test_pred)

    print(f"\næœ€ä¼˜æ¨¡å‹æ€§èƒ½:")
    print(f"  CV AUC (è®­ç»ƒé›†): {grid_search.best_score_:.4f}")
    print(f"  Train AUC (å…¨è®­ç»ƒé›†): {train_auc:.4f}")
    print(f"  Test AUC (æµ‹è¯•é›†): {test_auc:.4f}")
    print(f"  Gap: {(test_auc - train_auc):.4f}")

    # Save best model
    best_model_path = MODEL_DIR / "catboost_best_model.cbm"
    best_model.save_model(best_model_path)
    print(f"\nâœ… æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜: {best_model_path}")

    # Save grid search results
    results_df = pd.DataFrame(grid_search.cv_results_)
    results_path = MODEL_DIR / "catboost_grid_search_results.csv"
    results_df.to_csv(results_path, index=False)
    print(f"âœ… ç½‘æ ¼æœç´¢ç»“æœå·²ä¿å­˜: {results_path}")

    return {
        'best_params': grid_search.best_params_,
        'best_score': grid_search.best_score_,
        'train_auc': train_auc,
        'test_auc': test_auc,
        'search_time': search_time,
        'best_model': best_model
    }


def main():
    """
    Main function for CatBoost hyperparameter tuning
    """
    print("\n" + "=" * 80)
    print("CATBOOST è¶…å‚æ•°è°ƒä¼˜ - CATBOOST HYPERPARAMETER TUNING")
    print("=" * 80)

    # Load data
    X_train, X_test, y_train, y_test = load_and_prepare_data()

    # Test different iteration counts
    print("\n\n")
    print("â–ˆ" * 80)
    print("â–ˆ ç¬¬1éƒ¨åˆ†: æµ‹è¯•ä¸åŒè¿­ä»£æ¬¡æ•°")
    print("â–ˆ" * 80)
    iteration_results = test_iteration_counts(
        X_train, X_test, y_train, y_test,
        iterations_list=[550, 600, 650]
    )

    # Optional: Perform grid search (commented out to save time)
    # Uncomment if you want to perform full grid search
    """
    print("\n\n")
    print("â–ˆ" * 80)
    print("â–ˆ ç¬¬2éƒ¨åˆ†: ç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°ç»„åˆ")
    print("â–ˆ" * 80)
    grid_results = perform_grid_search(X_train, X_test, y_train, y_test)
    """

    print("\n" + "=" * 80)
    print("âœ… è¶…å‚æ•°è°ƒä¼˜å®Œæˆï¼")
    print("=" * 80)
    print(f"\næ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {MODEL_DIR}")
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  1. catboost_iteration_comparison.csv - è¿­ä»£æ¬¡æ•°å¯¹æ¯”ç»“æœ")
    print(f"  2. catboost_model_550iter.cbm - 550æ¬¡è¿­ä»£æ¨¡å‹")
    print(f"  3. catboost_model_600iter.cbm - 600æ¬¡è¿­ä»£æ¨¡å‹")
    print(f"  4. catboost_model_650iter.cbm - 650æ¬¡è¿­ä»£æ¨¡å‹")
    # print(f"  5. catboost_best_model.cbm - ç½‘æ ¼æœç´¢æœ€ä¼˜æ¨¡å‹")
    # print(f"  6. catboost_grid_search_results.csv - ç½‘æ ¼æœç´¢å®Œæ•´ç»“æœ")


if __name__ == "__main__":
    main()
